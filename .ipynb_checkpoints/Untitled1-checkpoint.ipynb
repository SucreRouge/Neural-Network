{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "torch.manual_seed(1) #reproducible\n",
    "EPOCH = 4\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist', #保存位置\n",
    "    train=True, #training set\n",
    "    transform=torchvision.transforms.ToTensor(), #converts a PIL.Image to torch.FloatTensor(C*H*W) in range(0.0,1.0)\n",
    "    download=True\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root='./MNIST',\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.38456159\n",
      "[1,   200] loss: 2.38904151\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2356ff26a8aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m                             \u001b[0mprev_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                             \u001b[0;31m#cal a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMAxg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mEMAg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mEMAx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                                 \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mEMAx_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mEMAx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(27)\n",
    "# network structure\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,D_in,H,D_out):\n",
    "        super(CNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(D_in,H)\n",
    "        #torch.nn.init.normal(self.fc1.weight, mean=0, std=0.01)\n",
    "        nn.init.xavier_normal(self.fc1.weight,gain = 1)\n",
    "        nn.init.constant(self.fc1.bias, 0.1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(H,D_out)\n",
    "        #torch.nn.init.normal(self.fc2.weight, mean=0, std=0.01)\n",
    "        nn.init.xavier_normal(self.fc2.weight, gain = 1)\n",
    "        nn.init.constant(self.fc2.bias, 0.1)\n",
    "       # self.out = nn.Linear(10,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x))\n",
    "        output = x\n",
    "        #output = self.out(x)\n",
    "        return output\n",
    "\n",
    "D_in,H,D_out = 784,10,10\n",
    "cnn = CNN(D_in,H,D_out)\n",
    "\n",
    "# initial hyperparameter\n",
    "learning_rate = 0.3\n",
    "u0 = 1\n",
    "#loss function:cross-entropy with l2 regularizaiton\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# inital all using viariables\n",
    "EMAg = []\n",
    "EMAg_2 = []\n",
    "EMAx = []\n",
    "EMAx_2 = []\n",
    "EMAxg = []\n",
    "EMAu = []\n",
    "beta = []\n",
    "\n",
    "prev_grad=[] #store params grad of last iter\n",
    "prev_x=[]    #store params of last iter\n",
    "\n",
    "# training iteration\n",
    "for epoch in range(EPOCH):\n",
    "    running_loss = 0.0                        # loss to show\n",
    "    #training each mini-batch in dataloader\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "       # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        cnn.zero_grad()\n",
    "\n",
    "        # forward + backward\n",
    "        outputs = cnn(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "       \n",
    "        # implementation of cSGD algo\n",
    "        for index,params in enumerate(cnn.parameters(),0):\n",
    "            \n",
    "            #update parameters process\n",
    "            if(i==0 and epoch==0):\n",
    "                #lists store EMA of weight_grad, weight_grad_square, weight, and weight_square\n",
    "                \n",
    "                EMAg.append(torch.Tensor(params.size()))\n",
    "                EMAx.append(torch.Tensor(params.size()))\n",
    "                EMAx_2.append(torch.Tensor(params.size()))\n",
    "                EMAg_2.append(torch.Tensor(params.size()))\n",
    "                EMAxg.append(torch.Tensor(params.size()))\n",
    "                prev_grad.append(torch.Tensor(params.size()))\n",
    "                prev_x.append(torch.Tensor(params.size()))\n",
    "                n = params.dim()\n",
    "                if n ==1:\n",
    "                    for item in range(0,params.size()[0]):\n",
    "                        EMAg[index][item] = params.grad.data[item]\n",
    "                        prev_grad[index][item] = params.grad.data[item]\n",
    "                        EMAx[index][item] = params.data[item]\n",
    "                        prev_x[index][item] = params.data[item]\n",
    "                        EMAg_2[index][item] = params.grad.data[item]**2\n",
    "                        EMAx_2[index][item] = params.data[item]**2\n",
    "                        EMAxg[index][item] = params.data[item]*params.grad.data[item]**2\n",
    "                    \n",
    "                if n ==2:\n",
    "                    for item1 in range(0,params.size()[0]):\n",
    "                        for item2 in range(0,params.size()[1]):\n",
    "                            EMAg[index][item1,item2] = params.grad.data[item1,item2]\n",
    "                            EMAx[index][item1,item2] = params.data[item1,item2]\n",
    "                            prev_grad[index][item1,item2] = params.grad.data[item1,item2]\n",
    "                            prev_x[index][item1,item2] = params.data[item1,item2]\n",
    "                            EMAg_2[index][item1,item2] = params.grad.data[item1,item2]**2\n",
    "                            EMAx_2[index][item1,item2] = params.data[item1,item2]**2\n",
    "                            EMAxg[index][item1,item2] = params.data[item1,item2]*params.grad.data[item1,item2]\n",
    "                \n",
    "                #lists store EMA of u and beta\n",
    "                EMAu.append(torch.Tensor(params.size()))\n",
    "                beta.append(torch.ones(params.size())*0.9)\n",
    "                EMAu[index] = torch.ones(EMAu[index].size())*u0\n",
    "                #print(EMAg[index][0,64])\n",
    "                #print(EMAg_2[index][0,1])\n",
    "            else:\n",
    "               \n",
    "                one = torch.ones(params.size())\n",
    "                #print(EMAg_2[index][0,1])\n",
    "                #update EMA\n",
    "                EMAg[index] = (beta[index])*EMAg[index]+(one-beta[index])*params.grad.data\n",
    "                EMAg_2[index] = (beta[index])*EMAg_2[index] + (one-beta[index])*(prev_grad[index]**2)\n",
    "                EMAx[index] = (beta[index])*EMAx[index] + (one-beta[index])*prev_x[index]\n",
    "                EMAx_2[index] = (beta[index])*EMAx_2[index]+(one-beta[index])*(prev_x[index]**2)\n",
    "                EMAxg[index] = (beta[index])*EMAxg[index] + (one-beta[index])*(prev_grad[index]*prev_x[index])\n",
    "                \n",
    "                \n",
    "                #cal a,b,sigma,u*\n",
    "                n = params.dim()\n",
    "                a = torch.Tensor(params.size())\n",
    "                b = torch.Tensor(params.size())\n",
    "                sigma = torch.Tensor(params.size())\n",
    "                u = torch.Tensor(params.size())\n",
    "            \n",
    "            \n",
    "                if n == 1:\n",
    "                    for item in range(0,params.size()[0]):\n",
    "                        prev_x[index][item] = params.data[item]\n",
    "                        prev_grad[index][item] = params.grad.data[item]\n",
    "                        #cal a\n",
    "                        if EMAxg[index][item]==EMAg[index][item]*EMAx[index][item]:\n",
    "                             a[item] = 0\n",
    "                        elif (EMAx_2[index][item]-EMAx[index][item]**2)==0:\n",
    "                             a[item] = 10000\n",
    "                        else:\n",
    "                             a[item] = (EMAxg[index][item]-EMAg[index][item]*EMAx[index][item])/(EMAx_2[index][item]-EMAx[index][item]**2)\n",
    "                        \n",
    "                        \n",
    "                        #cal sigma\n",
    "                        sigma[item] = EMAg_2[index][item] - EMAg[index][item]**2\n",
    "                        \n",
    "                        #cal u*\n",
    "                        if(a[item]<= 0):\n",
    "                            u[item] = 1\n",
    "                        else:\n",
    "                            if(EMAg[index][item]==0):\n",
    "                                u[item] = 0.0\n",
    "                            elif(sigma[item]==0 or a[item]==0):\n",
    "                                u[item] = 1.0\n",
    "                            else:\n",
    "                                u[item] = min(1,(EMAg[index][item]**2)/(learning_rate*sigma[item]*a[item])) \n",
    "                        \n",
    "                    \n",
    "                        #cal beta\n",
    "                        if (EMAg_2[index][item]==EMAg[index][item]**2):\n",
    "                            beta[index][item] = 0.9\n",
    "                        elif (EMAg_2[index][item]==0):\n",
    "                            beta[index][item] = 1000\n",
    "                        else:\n",
    "                            beta[index][item] = 0.9+(0.999-0.9)*(EMAg_2[index][item]-EMAg[index][item]**2)/(EMAg_2[index][item])\n",
    "                        #update EMA u\n",
    "                        EMAu[index][item] = (1-beta[index][item])*EMAu[index][item] + beta[index][item]*u[item]\n",
    "                if n == 2:\n",
    "                    for item1 in range(0,params.size()[0]):\n",
    "                        for item2 in range(0,params.size()[1]):\n",
    "                            prev_grad[index][item1,item2] = params.grad.data[item1,item2]\n",
    "                            prev_x[index][item1,item2] = params.data[item1,item2]\n",
    "                            #cal a\n",
    "                            if ((EMAxg[index][item1,item2]-EMAg[index][item1,item2]*EMAx[index][item1,item2])==0):\n",
    "                                a[item1,item2] = 0\n",
    "                            elif (EMAx_2[index][item1,item2]-EMAx[index][item1,item2]**2)==0:\n",
    "                                a[item1,item2] = 10000\n",
    "                            else:\n",
    "                                a[item1,item2] =(EMAxg[index][item1,item2]-EMAg[index][item1,item2]*EMAx[index][item1,item2])/(EMAx_2[index][item1,item2]-EMAx[index][item1,item2]**2)            \n",
    "                                #print(a[item1,item2])\n",
    "                        \n",
    "                            #cal sigma\n",
    "                            sigma[item1,item2] = EMAg_2[index][item1,item2] - math.pow(EMAg[index][item1,item2],2)\n",
    "                            #print(sigma[item1,item2])\n",
    "                            #cal u*\n",
    "                            if(a[item1,item2]<= 0):\n",
    "                                u[item1,item2] = 1.0\n",
    "                            else:\n",
    "                                if(EMAg[index][item1,item2]==0):\n",
    "                                    u[item1,item2] = 0.0\n",
    "                                elif(sigma[item1,item2]==0 or a[item1,item2]==0):\n",
    "                                    u[item1,item2] = 1.0\n",
    "                                else:\n",
    "                                    u[item1,item2] = min(1,(EMAg[index][item1,item2]**2)/(learning_rate*sigma[item1,item2]*a[item1,item2])) \n",
    "                        \n",
    "                                #print(a[item1,item2],b[item1,item2],sigma[item1,item2])\n",
    "                                #print(u[item1,item2])\n",
    "                                #print(a[item1,item2]*((EMAx[index][item1,item2]-b[item1,item2])**2)/(learning_rate*sigma[item1,item2]))\n",
    "                            \n",
    "                            #cal beta\n",
    "                            if EMAg_2[index][item1,item2]==EMAg[index][item1,item2]**2:\n",
    "                                beta[index][item1,item2] = 0.9\n",
    "                            elif (EMAg_2[index][item1,item2]==0):\n",
    "                                beta[index][item1,item2] = 1000\n",
    "                            else:\n",
    "                                beta[index][item1,item2] = 0.9+(0.999-0.9)*(EMAg_2[index][item1,item2]-EMAg[index][item1,item2]**2)/(EMAg_2[index][item1,item2])\n",
    "                            #update EMAu\n",
    "                            EMAu[index][item1,item2] = (1-beta[index][item1,item2])*EMAu[index][item1,item2] + (beta[index][item1,item2])*u[item1,item2]\n",
    "                            #print(EMAu[index][item1,item2])\n",
    "            #update weight and bias\n",
    "            #prev_grad[index] = params.grad.data\n",
    "            #prev_x[index] = params.data\n",
    "            params.data -= learning_rate *EMAu[index]* params.grad.data\n",
    "        \n",
    "        #print(\"%d %d\"%(epoch, i))\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            #print(EMAu[0])\n",
    "            print('[%d, %5d] loss: %.8f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
